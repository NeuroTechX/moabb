{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Within Session P300\n\nThis example shows how to perform a within session analysis on three different\nP300 datasets.\n\nWe will compare two pipelines :\n\n- Riemannian geometry\n- XDAWN with Linear Discriminant Analysis\n\nWe will use the P300 paradigm, which uses the AUC as metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Pedro Rodrigues <pedro.rodrigues01@gmail.com>\n#\n# License: BSD (3-clause)\n\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mne.decoding import Vectorizer\nfrom pyriemann.estimation import Xdawn, XdawnCovariances\nfrom pyriemann.tangentspace import TangentSpace\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.pipeline import make_pipeline\n\nimport moabb\nfrom moabb.datasets import BNCI2014_009\nfrom moabb.evaluations import WithinSessionEvaluation\nfrom moabb.paradigms import P300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "getting rid of the warnings about the future\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\nwarnings.simplefilter(action=\"ignore\", category=RuntimeWarning)\n\nmoabb.set_log_level(\"info\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Pipelines\n\nPipelines must be a dict of sklearn pipeline transformer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipelines = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have to do this because the classes are called 'Target' and 'NonTarget'\nbut the evaluation function uses a LabelEncoder, transforming them\nto 0 and 1\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "labels_dict = {\"Target\": 1, \"NonTarget\": 0}\n\npipelines[\"RG+LDA\"] = make_pipeline(\n    XdawnCovariances(\n        nfilter=2, classes=[labels_dict[\"Target\"]], estimator=\"lwf\", xdawn_estimator=\"scm\"\n    ),\n    TangentSpace(),\n    LDA(solver=\"lsqr\", shrinkage=\"auto\"),\n)\n\npipelines[\"Xdw+LDA\"] = make_pipeline(\n    Xdawn(nfilter=2, estimator=\"scm\"), Vectorizer(), LDA(solver=\"lsqr\", shrinkage=\"auto\")\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n\nWe define the paradigm (P300) and use all three datasets available for it.\nThe evaluation will return a DataFrame containing a single AUC score for\neach subject / session of the dataset, and for each pipeline.\n\nResults are saved into the database, so that if you add a new pipeline, it\nwill not run again the evaluation unless a parameter has changed. Results can\nbe overwritten if necessary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "paradigm = P300(resample=128)\ndataset = BNCI2014_009()\ndataset.subject_list = dataset.subject_list[:2]\ndatasets = [dataset]\noverwrite = True  # set to True if we want to overwrite cached results\nevaluation = WithinSessionEvaluation(\n    paradigm=paradigm, datasets=datasets, suffix=\"examples\", overwrite=overwrite\n)\nresults = evaluation.process(pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Results\n\nHere we plot the results to compare the two pipelines\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=[8, 4])\n\nsns.stripplot(\n    data=results,\n    y=\"score\",\n    x=\"pipeline\",\n    ax=ax,\n    jitter=True,\n    alpha=0.5,\n    zorder=1,\n    palette=\"Set1\",\n)\nsns.pointplot(data=results, y=\"score\", x=\"pipeline\", ax=ax, palette=\"Set1\")\n\nax.set_ylabel(\"ROC AUC\")\nax.set_ylim(0.5, 1)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}