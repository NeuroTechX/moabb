{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmarking on MOABB with Tensorflow deep net architectures\nThis example shows how to use MOABB to benchmark a set of Deep Learning pipeline (Tensorflow)\non all available datasets.\nFor this example, we will use only one dataset to keep the computation time low, but this benchmark is designed\nto easily scale to many datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Igor Carrara <igor.carrara@inria.fr>\n#\n# License: BSD (3-clause)\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom absl.logging import ERROR, set_verbosity\n\nfrom moabb import benchmark, set_log_level\nfrom moabb.analysis.plotting import score_plot\nfrom moabb.datasets import BNCI2014_001\nfrom moabb.utils import setup_seed\n\n\nset_log_level(\"info\")\n# Avoid output Warning\nset_verbosity(ERROR)\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\n\nCPU = len(tf.config.list_physical_devices(\"CPU\")) > 0\nprint(\"CPU is\", \"AVAILABLE\" if CPU else \"NOT AVAILABLE\")\n\nGPU = len(tf.config.list_physical_devices(\"GPU\")) > 0\nprint(\"GPU is\", \"AVAILABLE\" if GPU else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we will use only the dataset ``BNCI2014_001``.\n\n## Running the benchmark\n\nThe benchmark is run using the ``benchmark`` function. You need to specify the\nfolder containing the pipelines to use, the kind of evaluation and the paradigm\nto use. By default, the benchmark will use all available datasets for all\nparadigms listed in the pipelines. You could restrict to specific evaluation and\nparadigm using the ``evaluations`` and ``paradigms`` arguments.\n\nTo save computation time, the results are cached. If you want to re-run the\nbenchmark, you can set the ``overwrite`` argument to ``True``.\n\nIt is possible to indicate the folder to cache the results and the one to save\nthe analysis & figures. By default, the results are saved in the ``results``\nfolder, and the analysis & figures are saved in the ``benchmark`` folder.\n\nThis code is implemented to run on CPU. If you're using a GPU, do not use multithreading\n(i.e. set n_jobs=1)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set up reproducibility of Tensorflow\nsetup_seed(42)\n\n# Restrict this example only on the first two subject of BNCI2014_001\ndataset = BNCI2014_001()\ndataset.subject_list = dataset.subject_list[:2]\ndatasets = [dataset]\n\nresults = benchmark(\n    pipelines=\"./pipelines_DL\",\n    evaluations=[\"WithinSession\"],\n    paradigms=[\"LeftRightImagery\"],\n    include_datasets=datasets,\n    results=\"./results/\",\n    overwrite=False,\n    plot=False,\n    output=\"./benchmark/\",\n    n_jobs=-1,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The deep learning architectures implemented in MOABB are:\n- Shallow Convolutional Network [1]_\n- Deep Convolutional Network [1]_\n- EEGNet [2]_\n- EEGTCNet [3]_\n- EEGNex [4]_\n- EEGITNet [5]_\n\nBenchmark prints a summary of the results. Detailed results are saved in a\npandas dataframe, and can be used to generate figures. The analysis & figures\nare saved in the ``benchmark`` folder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score_plot(results)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. [1] Schirrmeister, R. T., Springenberg, J. T., Fiederer, L. D. J.,\n   Glasstetter, M., Eggensperger, K., Tangermann, M., ... & Ball, T. (2017).\n   [Deep learning with convolutional neural networks for EEG decoding and\n   visualization](https://doi.org/10.1002/hbm.23730).\n   Human brain mapping, 38(11), 5391-5420.\n.. [2] Lawhern, V. J., Solon, A. J., Waytowich, N. R., Gordon, S. M.,\n   Hung, C. P., & Lance, B. J. (2018). [EEGNet: a compact convolutional neural\n   network for EEG-based brain-computer interfaces.](https://doi.org/10.1088/1741-2552/aace8c)\n   Journal of neural engineering, 15(5), 056013.\n.. [3] Ingolfsson, T. M., Hersche, M., Wang, X., Kobayashi, N., Cavigelli, L., &\n   Benini, L. (2020, October). [EEG-TCNet: An accurate temporal convolutional\n   network for embedded motor-imagery brain-machine interfaces.](https://doi.org/10.1109/SMC42975.2020.9283028)\n   In 2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)\n   (pp. 2958-2965). IEEE.\n.. [4] Chen, X., Teng, X., Chen, H., Pan, Y., & Geyer, P. (2022). [Toward reliable\n   signals decoding for electroencephalogram: A benchmark study to EEGNeX.](https://doi.org/10.48550/arXiv.2207.12369)\n   arXiv preprint arXiv:2207.12369.\n.. [5] Salami, A., Andreu-Perez, J., & Gillmeister, H. (2022). [EEG-ITNet: An\n   explainable inception temporal convolutional network for motor imagery\n   classification](https://doi.org/10.1109/ACCESS.2022.3161489).\n   IEEE Access, 10, 36672-36685.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}