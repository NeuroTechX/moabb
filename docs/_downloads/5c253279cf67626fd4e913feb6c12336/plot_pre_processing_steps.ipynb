{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Playing with the pre-processing steps\nBy default, MOABB uses **fundamental** and **robust** pre-processing steps defined in\neach paradigm.\n\nBehind the curtains, these steps are defined in a scikit-learn Pipeline.\nThis pipeline receives raw signals and applies various signal processing steps\nto construct the final array object and class labels, which will be used\nto train and evaluate the classifiers.\n\nPre-processing steps are known to shape the rank and\nmetric results of the EEG Decoding [2]_, [3]_, [4]_,\nand we present some discussion in our largest benchmark paper [1]_\non why we used those specific steps.\nUsing the same pre-processing steps for all datasets also avoids biases\nand makes results more comparable.\n\nHowever, there might be cases where these steps are not adequate.\nMOABB allows you to modify the pre-processing pipeline.\nIn this example, we will show how to use the `make_process_pipelines` method to create a\ncustom pre-processing pipeline. We will use the MinMaxScaler from `sklearn` to scale the\ndata channels to the range [0, 1].\n\n## References\n.. [1] Chevallier, S., Carrara, I., Aristimunha, B., Guetschel, P., Sedlar, S., Lopes, B., ... & Moreau, T. (2024). The largest EEG-based BCI reproducibility study for open science: the MOABB benchmark. arXiv preprint arXiv:2404.15319.\n\n.. [2] Kessler, R., Enge, A., & Skeide, M. A. (2024). How EEG preprocessing shapes decoding performance. arXiv preprint arXiv:2410.14453.\n\n.. [3] Delorme, A. (2023). EEG is better left alone. Scientific reports, 13(1), 2372.\n\n.. [4] Clayson, P. E. (2024). Beyond single paradigms, pipelines, and outcomes: Embracing multiverse analyses in psychophysiology. International Journal of Psychophysiology, 197, 112311.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Bruno Aristimunha Pinto <b.aristimunha@gmail.com>\n#\n# License: BSD (3-clause)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is applied precisely to each paradigm?\n\nEach paradigm defines a set of pre-processing steps that are applied to the raw data\nin order to construct the numpy arrays and class labels used for classification.\nIn MOABB, the pre-processing steps are divided into three groups:\nthe steps which are applied over the `raw` objects, those applied to the `epoch` objects,\nand those for  the `array` objects.\n\nFirst things, let's define one dataset and one paradigm.\nHere, we will use the BNCI2014_001 dataset and the LeftRightImagery paradigm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom moabb.datasets import BNCI2014_001\nfrom moabb.datasets.bids_interface import StepType\nfrom moabb.evaluations import CrossSessionEvaluation\nfrom moabb.paradigms import FilterBankLeftRightImagery, LeftRightImagery\n\n\ndataset = BNCI2014_001()\n# Select one subject for the example. You can use the dataset for all subjects\ndataset.subject_list = dataset.subject_list[:1]\n\nparadigm = LeftRightImagery()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exposing the pre-processing steps\n\nThe most efficient way to expose the pre-processing steps is to use the\n`make_process_pipelines` method. This method will return a list of pipelines that\nare applied to the raw data. The pipelines are defined in the paradigm object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "process_pipeline = paradigm.make_process_pipelines(dataset)\n\n# On the not filterbank paradigm, we have only one branch of possible steps steps:\nprocess_pipeline[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filter Bank Paradigm\n\nOn the filterbank paradigm, we have n branches in the case of multiple filters:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "paradigm_filterbank = FilterBankLeftRightImagery()\npre_procesing_filter_bank_steps = paradigm_filterbank.make_process_pipelines(dataset)\n\n# By default, we have six filter banks, and each filter bank has the same steps.\nfor i, step in enumerate(pre_procesing_filter_bank_steps):\n    print(f\"Filter bank {i}: {step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How to include extra steps?\n -------------------------------\n\n The paradigm object accepts parameters to configure common\n pre-processing and epoching steps applied to the raw data. These include:\n\n - Bandpass filtering (`filters`)\n - Event selection for epoching (`events`)\n - Epoch time window definition (`tmin`, `tmax`)\n - Baseline correction (`baseline`)\n - Channel selection (`channels`)\n - Resampling (`resample`)\n\n The following example demonstrates how you can surgically add custom processing steps\n beyond these built-in options.\n\n In this example, we want to add a min-max function step to the raw data to do this.\n We need to do pipeline surgery and use the evaluation function.\n#############################################################################\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "process_pipeline = paradigm.make_process_pipelines(dataset)[0]\n\nprocess_pipeline.steps.insert(2, (StepType.RAW, MinMaxScaler()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that you have defined some special pre-processing, you will need to run with\n`evaluation` function to get the results.\nHere, we will use the `DummyClassifier` from sklearn to run the evaluation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "classifier_pipeline = {}\nclassifier_pipeline[\"dummy\"] = DummyClassifier()\n\nevaluation = CrossSessionEvaluation(paradigm=paradigm)\n\ngenerator_results = evaluation.evaluate(\n    dataset=dataset,\n    pipelines=classifier_pipeline,\n    param_grid=None,\n    process_pipeline=process_pipeline,\n)\n# The evaluation function will return a generator object that contains the results\n# of the evaluation. You can use the `list` function to convert it to a list.\nresults = list(generator_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Results\n\nThen you can follow the common procedure for analyzing the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame(results)\n\ndf_results.plot(\n    x=\"pipeline\",\n    y=\"score\",\n    kind=\"bar\",\n    title=\"Results of the evaluation with custom pre-processing steps\",\n    xlabel=\"Pipeline\",\n    ylabel=\"Score\",\n)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}