{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmarking with MOABB with Grid Search\n\nThis example shows how to use MOABB to benchmark a set of pipelines\non all available datasets. In particular we run the Gridsearch to select the best hyperparameter of some pipelines\nand save the gridsearch.\nFor this example, we will use only one dataset to keep the computation time low, but this benchmark is designed\nto easily scale to many datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Igor Carrara <igor.carrara@inria.fr>\n#\n# License: BSD (3-clause)\n\nimport matplotlib.pyplot as plt\n\nfrom moabb import benchmark, set_log_level\nfrom moabb.analysis.plotting import score_plot\n\n\nset_log_level(\"info\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we will use only the dataset 'Zhou 2016'.\n\n## Running the benchmark\n\nThe benchmark is run using the ``benchmark`` function. You need to specify the\nfolder containing the pipelines to use, the kind of evaluation and the paradigm\nto use. By default, the benchmark will use all available datasets for all\nparadigms listed in the pipelines. You could restrict to specific evaluation and\nparadigm using the ``evaluations`` and ``paradigms`` arguments.\n\nTo save computation time, the results are cached. If you want to re-run the\nbenchmark, you can set the ``overwrite`` argument to ``True``.\n\nIt is possible to indicate the folder to cache the results and the one to save\nthe analysis & figures. By default, the results are saved in the ``results``\nfolder, and the analysis & figures are saved in the ``benchmark`` folder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# In the results folder we will save the gridsearch evaluation\n# When write the pipeline in ylm file we need to specify the parameter that we want to test, in format\n# pipeline-name__estimator-name_parameter. Note that pipeline and estimator names MUST\n# be in lower case (no capital letters allowed).\n# If the grid search is already implemented it will load the previous results\n\nresults = benchmark(\n    pipelines=\"./pipelines_grid/\",\n    evaluations=[\"WithinSession\"],\n    paradigms=[\"LeftRightImagery\"],\n    include_datasets=[\"Zhou2016\"],\n    results=\"./results/\",\n    overwrite=False,\n    plot=False,\n    output=\"./benchmark/\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark prints a summary of the results. Detailed results are saved in a\npandas dataframe, and can be used to generate figures. The analysis & figures\nare saved in the ``benchmark`` folder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score_plot(results)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}