{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Examples of how to use MOABB to benchmark pipelines.\nBenchmarking with MOABB\n=======================\n\nThis example shows how to use MOABB to benchmark a set of pipelines\non all available datasets. For this example, we will use only one\ndataset to keep the computation time low, but this benchmark is designed\nto easily scale to many datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Sylvain Chevallier <sylvain.chevallier@universite-paris-saclay.fr>\n#\n# License: BSD (3-clause)\n\nimport matplotlib.pyplot as plt\n\nfrom moabb import benchmark, set_log_level\nfrom moabb.analysis.plotting import score_plot\nfrom moabb.paradigms import LeftRightImagery\n\n\nset_log_level(\"info\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the pipelines\n\nThe ML pipelines used in benchmark are defined in YAML files, following a\nsimple format. It simplifies sharing and reusing pipelines across benchmarks,\nreproducing state-of-the-art results.\n\nMOABB comes with complete list of pipelines that cover most of the successful\napproaches in the literature. You can find them in the\n[pipelines folder](https://github.com/NeuroTechX/moabb/tree/develop/pipelines).\nFor this example, we will use a folder with only 2 pipelines, to keep the\ncomputation time low.\n\nThis is an example of a pipeline defined in YAML, defining on which paradigms it\ncan be used, the original publication, and the steps to perform using a\nscikit-learn API. In this case, a CSP + SVM pipeline, the covariance are estimated\nto compute a CSP filter and then a linear SVM is trained on the CSP filtered\nsignals.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with open(\"sample_pipelines/CSP_SVM.yml\", \"r\") as f:\n    lines = f.readlines()\n    for line in lines:\n        print(line, end=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``sample_pipelines`` folder contains a second pipeline, a logistic regression\nperformed in the tangent space using Riemannian geometry.\n\n## Selecting the datasets (optional)\n\nIf you want to limit your benchmark on a subset of datasets, you can use the\n``include_datasets`` and ``exclude_datasets`` arguments. You will need either\nto provide the dataset's object, or a the dataset's code. To get the list of\navailable dataset's code for a given paradigm, you can use the following command:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "paradigm = LeftRightImagery()\nfor d in paradigm.datasets:\n    print(d.code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we will use only the last dataset, 'Zhou 2016'.\n\n## Running the benchmark\n\nThe benchmark is run using the ``benchmark`` function. You need to specify the\nfolder containing the pipelines to use, the kind of evaluation and the paradigm\nto use. By default, the benchmark will use all available datasets for all\nparadigms listed in the pipelines. You could restrict to specific evaluation and\nparadigm using the ``evaluations`` and ``paradigms`` arguments.\n\nTo save computation time, the results are cached. If you want to re-run the\nbenchmark, you can set the ``overwrite`` argument to ``True``.\n\nIt is possible to indicate the folder to cache the results and the one to save\nthe analysis & figures. By default, the results are saved in the ``results``\nfolder, and the analysis & figures are saved in the ``benchmark`` folder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = benchmark(\n    pipelines=\"./sample_pipelines/\",\n    evaluations=[\"WithinSession\"],\n    paradigms=[\"LeftRightImagery\"],\n    include_datasets=[\"Zhou2016\"],\n    results=\"./results/\",\n    overwrite=False,\n    plot=False,\n    output=\"./benchmark/\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark prints a summary of the results. Detailed results are saved in a\npandas dataframe, and can be used to generate figures. The analysis & figures\nare saved in the ``benchmark`` folder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "score_plot(results)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}