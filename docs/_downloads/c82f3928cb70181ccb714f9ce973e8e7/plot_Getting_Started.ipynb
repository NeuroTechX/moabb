{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tutorial 0: Getting Started\n\nThis tutorial takes you through a basic working example of how to use this\ncodebase, including all the different components, up to the results\ngeneration. If you'd like to know about the statistics and plotting, see the\nnext tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Vinay Jayaram <vinayjayaram13@gmail.com>\n#\n# License: BSD (3-clause)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\nTo use the codebase you need an evaluation and a paradigm, some algorithms,\nand a list of datasets to run it all on. You can find those in the following\nsubmodules; detailed tutorials are given for each of them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you would like to specify the logging level when it is running, you can\nuse the standard python logging commands through the top-level moabb module\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import moabb\nfrom moabb.datasets import BNCI2014_001, utils\nfrom moabb.evaluations import CrossSessionEvaluation\nfrom moabb.paradigms import LeftRightImagery\nfrom moabb.pipelines.features import LogVariance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to create pipelines within a script, you will likely need at least\nthe make_pipeline function. They can also be specified via a .yml file. Here\nwe will make a couple pipelines just for convenience\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "moabb.set_log_level(\"info\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create pipelines\n\nWe create two pipelines: channel-wise log variance followed by LDA, and\nchannel-wise log variance followed by a cross-validated SVM (note that a\ncross-validation via scikit-learn cannot be described in a .yml file). For\nlater in the process, the pipelines need to be in a dictionary where the key\nis the name of the pipeline and the value is the Pipeline object\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipelines = {}\npipelines[\"AM+LDA\"] = make_pipeline(LogVariance(), LDA())\nparameters = {\"C\": np.logspace(-2, 2, 10)}\nclf = GridSearchCV(SVC(kernel=\"linear\"), parameters)\npipe = make_pipeline(LogVariance(), clf)\n\npipelines[\"AM+SVM\"] = pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Datasets\n\nDatasets can be specified in many ways: Each paradigm has a property\n'datasets' which returns the datasets that are appropriate for that paradigm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(LeftRightImagery().datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or you can run a search through the available datasets:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(utils.dataset_search(paradigm=\"imagery\", min_subjects=6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or you can simply make your own list (which we do here due to computational\nconstraints)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = BNCI2014_001()\ndataset.subject_list = dataset.subject_list[:2]\ndatasets = [dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paradigm\n\nParadigms define the events, epoch time, bandpass, and other preprocessing\nparameters. They have defaults that you can read in the documentation, or you\ncan simply set them as we do here. A single paradigm defines a method for\ngoing from continuous data to trial data of a fixed size. To learn more look\nat the tutorial Exploring Paradigms\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fmin = 8\nfmax = 35\nparadigm = LeftRightImagery(fmin=fmin, fmax=fmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n\nAn evaluation defines how the training and test sets are chosen. This could\nbe cross-validated within a single recording, or across days, or sessions, or\nsubjects. This also is the correct place to specify multiple threads.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluation = CrossSessionEvaluation(\n    paradigm=paradigm, datasets=datasets, suffix=\"examples\", overwrite=False\n)\nresults = evaluation.process(pipelines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Results are returned as a pandas DataFrame, and from here you can do as you\nwant with them\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(results.head())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}