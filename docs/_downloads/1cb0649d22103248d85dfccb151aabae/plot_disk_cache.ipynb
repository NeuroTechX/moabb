{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Cache on disk intermediate data processing states\n\nThis example shows how intermediate data processing\nstates can be cached on disk to speed up the loading\nof this data in subsequent calls.\n\nWhen a MOABB paradigm processes a dataset, it will\nfirst apply processing steps to the raw data, this is\ncalled the ``raw_pipeline``. Then, it will convert the\nraw data into epochs and apply processing steps on the\nepochs, this is called the ``epochs_pipeline``.\nFinally, it will eventually convert the epochs into arrays,\nthis is called the ``array_pipeline``. In summary:\n\n``raw_pipeline`` --> ``epochs_pipeline`` --> ``array_pipeline``\n\nAfter each step, MOABB offers the possibility to save on disk\nthe result of the step. This is done by setting the ``cache_config``\nparameter of the paradigm's ``get_data`` method.\nThe ``cache_config`` parameter is a dictionary that can take all\nthe parameters of ``moabb.datasets.base.CacheConfig`` as keys,\nthey are the following: ``use``, ``save_raw``, ``save_epochs``,\n``save_array``, ``overwrite_raw``, ``overwrite_epochs``,\n``overwrite_array``, and ``path``.  You can also directly pass a\n``CacheConfig`` object as ``cache_config``.\n\nIf ``use=False``, the ``save_*`` and ``overwrite_*``\nparameters are ignored.\n\nWhen trying to use the cache (i.e. ``use=True``), MOABB will\nfirst check if there exist a cache of the result of the full\npipeline (i.e. ``raw_pipeline`` --> ``epochs_pipeline`` ->\n``array_pipeline``).\nIf there is none, we remove the last step of the pipeline and\nlook for its cached result. We keep removing steps and looking\nfor a cached result until we find one or until we reach an\nempty pipeline.\nEvery time, if the ``overwrite_*`` parameter\nof the corresponding step is true, we first try to erase the\ncache of this step.\nOnce a cache has been found or the empty pipeline has been reached,\ndepending on the case we either load the cache or the original dataset.\nThen, apply the missing steps one by one and save their result\nif their corresponding ``save_*`` parameter is true.\n\nBy default, only the result of the ``raw_pipeline`` is saved.\nThis is usually a good compromise between speed and disk space\nbecause, when using cached raw data, the epochs can be obtained\nwithout preloading the whole raw signals, only the necessary\nintervals. Yet, because only the raw data is cached, the epoching\nparameters can be changed without creating a new cache each time.\nHowever, if your epoching parameters are fixed, you can directly\ncache the epochs or the arrays to speed up the loading and\nreduce the disk space used.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>The ``cache_config`` parameter is also available for the ``get_data``\n    method of the datasets. It works the same way as for a\n    paradigm except that it will save un-processed raw recordings.</p></div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Pierre Guetschel <pierre.guetschel@gmail.com>\n#\n# License: BSD (3-clause)\n\nimport shutil\nimport tempfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nfrom pathlib import Path\n\nfrom moabb import set_log_level\nfrom moabb.datasets import Zhou2016\nfrom moabb.paradigms import LeftRightImagery\n\n\nset_log_level(\"info\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic usage\n\nThe ``cache_config`` parameter is a dictionary that has the\nfollowing default values:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "default_cache_config = dict(\n    save_raw=False,\n    save_epochs=False,\n    save_array=False,\n    use=False,\n    overwrite_raw=False,\n    overwrite_epochs=False,\n    overwrite_array=False,\n    path=None,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You don not need to specify all the keys of ``cache_config``, only the ones\nyou want to change.\n\nBy default, the cache is saved at the MNE data directory (i.e. when\n``path=None``).  The MNE data directory can be found with\n``mne.get_config('MNE_DATA')``. For this example, we will save it  in a\ntemporary directory instead:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "temp_dir = Path(tempfile.mkdtemp())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use the Zhou2016 dataset and the LeftRightImagery paradigm in this\nexample, but this works for any dataset and paradigm pair.:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = Zhou2016()\nparadigm = LeftRightImagery()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we will only use the first subject for this example:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subjects = [1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, saving a cache can simply be done by setting the desired parameters\nin the ``cache_config`` dictionary:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cache_config = dict(\n    use=True,\n    save_raw=True,\n    save_epochs=True,\n    save_array=True,\n    path=temp_dir,\n)\n_ = paradigm.get_data(dataset, subjects, cache_config=cache_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time comparison\n\nNow, we will compare the time it takes to load the with different levels of\ncache. For this, we will use the cache saved in the previous block and\noverwrite the steps results one by one so that we can compare the time it\ntakes to load the data and compute the missing steps with an increasing\nnumber of missing steps.\n\nUsing array cache:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cache_config = dict(\n    use=True,\n    path=temp_dir,\n    save_raw=False,\n    save_epochs=False,\n    save_array=False,\n    overwrite_raw=False,\n    overwrite_epochs=False,\n    overwrite_array=False,\n)\nt0 = time.time()\n_ = paradigm.get_data(dataset, subjects, cache_config=cache_config)\nt_array = time.time() - t0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using epochs cache:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cache_config = dict(\n    use=True,\n    path=temp_dir,\n    save_raw=False,\n    save_epochs=False,\n    save_array=False,\n    overwrite_raw=False,\n    overwrite_epochs=False,\n    overwrite_array=True,\n)\nt0 = time.time()\n_ = paradigm.get_data(dataset, subjects, cache_config=cache_config)\nt_epochs = time.time() - t0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using raw cache:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cache_config = dict(\n    use=True,\n    path=temp_dir,\n    save_raw=False,\n    save_epochs=False,\n    save_array=False,\n    overwrite_raw=False,\n    overwrite_epochs=True,\n    overwrite_array=True,\n)\nt0 = time.time()\n_ = paradigm.get_data(dataset, subjects, cache_config=cache_config)\nt_raw = time.time() - t0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using no cache:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cache_config = dict(\n    use=False,\n    path=temp_dir,\n    save_raw=False,\n    save_epochs=False,\n    save_array=False,\n    overwrite_raw=False,\n    overwrite_epochs=False,\n    overwrite_array=False,\n)\nt0 = time.time()\n_ = paradigm.get_data(dataset, subjects, cache_config=cache_config)\nt_nocache = time.time() - t0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Time needed to load the data with different levels of cache:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Using array cache: {t_array:.2f} seconds\")\nprint(f\"Using epochs cache: {t_epochs:.2f} seconds\")\nprint(f\"Using raw cache: {t_raw:.2f} seconds\")\nprint(f\"Without cache: {t_nocache:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, using a raw cache is more than 5 times faster than\nwithout cache.\nThis is because when using the raw cache, the data is not preloaded, only\nthe desired epochs are loaded in memory.\n\nUsing the epochs cache is a little faster than the raw cache. This is because\nthere are several preprocessing steps done after the epoching by the\n``epochs_pipeline``. This difference would be greater if the ``resample``\nargument was different that the sampling frequency of the dataset. Indeed,\nthe data loading time is directly proportional to its sampling frequency\nand the resampling is done by the ``epochs_pipeline``.\n\nFinally, we observe very little difference between array and epochs cache.\nThe main interest of the array cache is when the user passes a\ncomputationally heavy but fixed additional preprocessing (for example\ncomputing the covariance matrices of the epochs). This can be done by using\nthe ``postprocess_pipeline`` argument. The output of this additional pipeline\n(necessary a numpy array) will be saved to avoid re-computing it each time.\n\n\n## Technical details\n\nUnder the hood, the cache is saved on disk in a Brain Imaging Data Structure\n(BIDS) compliant format. More details on this structure can be found in the\ntutorial :doc:`./plot_bids_conversion`.\n\nHowever, there are two particular aspects of the way MOABB saves the data\nthat are not specific to BIDS:\n\n* For each file, we set a\n  [description key](https://bids-specification.readthedocs.io/en/stable/appendices/entities.html#desc).\n  This key is a code that corresponds to a hash of the\n  pipeline that was used to generate the data (i.e. from raw to the state\n  of the cache). This code is unique for each different pipeline and allows\n  to identify all the files that were generated by the same pipeline.\n* Once we finish saving all the files for a given combination of dataset,\n  subject, and pipeline, we write a file ending in ``\"_lockfile.json\"`` at\n  the root directory of this subject. This file serves two purposes:\n\n  * It indicates that the cache is complete for this subject and pipeline.\n    If it is not present, it means that something went wrong during the\n    saving process and the cache is incomplete.\n  * The file contains the un-hashed string representation of the pipeline.\n    Therefore, it can be used to identify the pipeline used without having\n    to decode the description key.\n\n## Cleanup\n\nFinally, we can delete the temporary folder:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(temp_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}