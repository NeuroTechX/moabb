{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Tutorial 1: Simple Motor Imagery\n\nIn this example, we will go through all the steps to make a simple BCI\nclassification task, downloading a dataset and using a standard classifier. We\nchoose the dataset 2a from BCI Competition IV, a motor imagery task. We will\nuse a CSP to enhance the signal-to-noise ratio of the EEG epochs and a LDA to\nclassify these signals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Authors: Pedro L. C. Rodrigues, Sylvain Chevallier\n#\n# https://github.com/plcrodrigues/Workshop-MOABB-BCI-Graz-2019\n\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom mne.decoding import CSP\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.pipeline import make_pipeline\n\nimport moabb\nfrom moabb.datasets import BNCI2014_001\nfrom moabb.evaluations import WithinSessionEvaluation\nfrom moabb.paradigms import LeftRightImagery\n\n\nmoabb.set_log_level(\"info\")\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiating Dataset\n\nThe first thing to do is to instantiate the dataset that we want to analyze.\nMOABB has a list of many different datasets, each one containing all the\nnecessary information for describing them, such as the number of subjects,\nsize of trials, names of classes, etc.\n\nThe dataset class has methods for:\n\n- downloading its files from some online source (e.g. Zenodo)\n- importing the data from the files in whatever extension they might be\n  (like .mat, .gdf, etc.) and instantiate a Raw object from the MNE package\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = BNCI2014_001()\ndataset.subject_list = [1, 2, 3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accessing EEG Recording\n\nAs an example, we may access the EEG recording from a given session and a\ngiven run as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sessions = dataset.get_data(subjects=[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This returns a MNE Raw object that can be manipulated. This might be enough\nfor some users, since the pre-processing and epoching steps can be easily\ndone via MNE. However, to conduct an assessment of several classifiers on\nmultiple subjects, MOABB ends up being a more appropriate option.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subject = 1\nsession_name = \"0train\"\nrun_name = \"0\"\nraw = sessions[subject][session_name][run_name]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing a Paradigm\n\nOnce we have instantiated a dataset, we have to choose a paradigm. This\nobject is responsible for filtering the data, epoching it, and extracting\nthe labels for each epoch. Note that each dataset comes with the names of\nthe paradigms to which it might be associated. It would not make sense to\nprocess a P300 dataset with a MI paradigm object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(dataset.paradigm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the example below, we will consider the paradigm associated to\nleft-hand/right-hand motor imagery task, but there are other options in\nMOABB for motor imagery, P300 or SSVEP.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "paradigm = LeftRightImagery()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We may check the list of all datasets available in MOABB for using with this\nparadigm (note that BNCI2014_001 is in it)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(paradigm.datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data from a list of subjects could be preprocessed and return as a 3D\nnumpy array `X`, follow a scikit-like format with the associated `labels`.\nThe `meta` object contains all information regarding the subject, the\nsession and the run associated to each trial.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, labels, meta = paradigm.get_data(dataset=dataset, subjects=[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Pipeline\n\nOur goal is to evaluate the performance of a given classification pipeline\n(or several of them) when it is applied to the epochs from the previously\nchosen dataset. We will consider a very simple classification pipeline in\nwhich the dimension of the epochs are reduced via a CSP step and then\nclassified via a linear discriminant analysis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline = make_pipeline(CSP(n_components=8), LDA())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation\n\nTo evaluate the score of this pipeline, we use the `evaluation` class. When\ninstantiating it, we say which paradigm we want to consider, a list with the\ndatasets to analyze, and whether the scores should be recalculated each time\nwe run the evaluation or if MOABB should create a cache file.\n\nNote that there are different ways of evaluating a classifier; in this\nexample, we choose `WithinSessionEvaluation`, which consists of doing a\ncross-validation procedure where the training and testing partitions are from\nthe same recording session of the dataset. We could have used\n`CrossSessionEvaluation`, which takes all but one session as training\npartition and the remaining one as testing partition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "evaluation = WithinSessionEvaluation(\n    paradigm=paradigm,\n    datasets=[dataset],\n    overwrite=True,\n    hdf5_path=None,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We obtain the results in the form of a pandas dataframe\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = evaluation.process({\"csp+lda\": pipeline})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results are stored in locally, to avoid recomputing the results each time.\nIt is saved in `hdf5_path` if defined or in ~/mne_data/results  otherwise.\nTo export the results in CSV:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results.to_csv(\"./results_part2-1.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To load previously obtained results saved in CSV\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = pd.read_csv(\"./results_part2-1.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting Results\n\nWe create a figure with the seaborn package comparing the classification\nscore for each subject on each session. Note that the 'subject' field from\nthe `results` is given in terms of integers, but seaborn accepts only\nstrings for its labeling. This is why we create the field 'subj'.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 7))\nresults[\"subj\"] = results[\"subject\"].apply(str)\nsns.barplot(\n    x=\"score\", y=\"subj\", hue=\"session\", data=results, orient=\"h\", palette=\"viridis\", ax=ax\n)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}